# Replikacja PostgreSQL i Wsp√≥≈Çdzielony Volume PlusWorkflow

## üìã PrzeglƒÖd RozwiƒÖza≈Ñ

### 1. PostgreSQL - Replikacja bez Operatora

**Obecna sytuacja:** Masz 3 repliki PostgreSQL, ale wszystkie sƒÖ `primary` - nie ma prawdziwej replikacji danych.

**Proponowane rozwiƒÖzania (od najprostszego):**

#### Opcja A: CloudNativePG Operator (ZALECANE dla produkcji)
- ‚úÖ Prostszy ni≈º Zalando Operator
- ‚úÖ Automatyczny failover
- ‚úÖ Backup i restore wbudowane
- ‚úÖ Streaming replication out-of-the-box
- ‚ö†Ô∏è Wymaga instalacji operatora

**Instalacja:**
```bash
kubectl apply -f https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.22/releases/cnpg-1.22.0.yaml
```

**Przyk≈Çadowy manifest:**
```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
  namespace: plusworkflow
spec:
  instances: 3
  postgresql:
    parameters:
      max_connections: "200"
  storage:
    size: 20Gi
    storageClass: local-path
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
```

#### Opcja B: Prosty Streaming Replication (manifest-08-postgres-replication-simple.yaml)
- ‚úÖ Bez operatora
- ‚úÖ Prostsza konfiguracja
- ‚ö†Ô∏è Rƒôczne zarzƒÖdzanie failover
- ‚ö†Ô∏è Wymaga dodatkowej konfiguracji u≈ºytkownika replicator

**Uwaga:** Ten manifest wymaga:
1. Utworzenia u≈ºytkownika replicator w PostgreSQL:
```sql
CREATE USER replicator WITH REPLICATION PASSWORD 'replicator_password';
```

2. Aktualizacji secret z has≈Çem replicator:
```yaml
# W manifest-02-postgres-secret.yaml dodaj:
stringData:
  replicator_password: replicator_password
```

#### Opcja C: Zalando Postgres Operator
- ‚úÖ Pe≈Çna funkcjonalno≈õƒá
- ‚ö†Ô∏è Bardzo skomplikowany
- ‚ö†Ô∏è Wymaga du≈ºo zasob√≥w
- ‚ö†Ô∏è Overkill dla ma≈Çych/≈õrednich ≈õrodowisk

**Rekomendacja:** CloudNativePG (Opcja A) - najlepszy balans prostoty i funkcjonalno≈õci.

---

### 2. PlusWorkflow Home - Wsp√≥≈Çdzielony Volume

**Problem:** Obecnie ka≈ºda instancja PlusWorkflow ma w≈Çasny volume (ReadWriteOnce), wiƒôc nie wsp√≥≈ÇdzielƒÖ danych.

**RozwiƒÖzanie:** Zmiana na `ReadWriteMany` (RWX) - ju≈º zaktualizowane w `manifest-07-plusworkflow-statefulset.yaml`.

**‚ö†Ô∏è WA≈ªNE:** Storage class `local-path` **NIE obs≈Çuguje** ReadWriteMany!

**Musisz zainstalowaƒá NFS provisioner:**

#### Instalacja NFS Client Provisioner (dla k3s)

```bash
# 1. Zainstaluj NFS provisioner
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --set nfs.server=<TW√ìJ_NFS_SERVER> \
  --set nfs.path=/exported/path \
  --set storageClass.name=nfs-client \
  --set storageClass.defaultClass=true

# 2. Sprawd≈∫ czy storage class zosta≈Ç utworzony
kubectl get storageclass nfs-client
```

**Alternatywy dla NFS:**
- **Longhorn** (dla k3s) - obs≈Çuguje RWX, ≈Çatwa instalacja
- **CephFS** - bardziej skomplikowany, ale bardzo wydajny
- **GlusterFS** - alternatywa dla NFS

**Instalacja Longhorn (najprostsze dla k3s):**
```bash
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/deploy/longhorn.yaml

# Po instalacji zmie≈Ñ storageClassName w manifest-07 na: longhorn
```

---

### 3. Ochrona Danych - Backupy

**Utworzony manifest:** `manifest-09-postgres-backup-cronjob.yaml`

**Funkcjonalno≈õci:**
- ‚úÖ Automatyczne backupy codziennie o 2:00
- ‚úÖ Kompresja (gzip)
- ‚úÖ Automatyczne usuwanie backup√≥w starszych ni≈º 30 dni
- ‚úÖ Rƒôczny backup na ≈ºƒÖdanie (Job)

**U≈ºycie:**

```bash
# Zastosuj backupy
kubectl apply -f manifest-09-postgres-backup-cronjob.yaml

# Sprawd≈∫ status CronJob
kubectl get cronjob -n plusworkflow

# Zobacz historiƒô backup√≥w
kubectl get jobs -n plusworkflow | grep postgres-backup

# Rƒôczny backup
kubectl create job --from=cronjob/postgres-backup postgres-backup-now -n plusworkflow

# Lista backup√≥w (w podzie)
kubectl exec -it $(kubectl get pod -n plusworkflow -l job-name=postgres-backup-manual -o jsonpath='{.items[0].metadata.name}') -- ls -lh /backups
```

**Przywracanie z backupu:**
```bash
# Skopiuj backup z poda
kubectl cp <pod-name>:/backups/plusworkflow_20240101_020000.sql.gz ./backup.sql.gz -n plusworkflow

# Przywr√≥ƒá
kubectl exec -it postgres-0 -n plusworkflow -- \
  sh -c 'gunzip < /path/to/backup.sql.gz | psql -U postgres -d plusworkflow'
```

---

### 4. Sticky Sessions - LoadBalancer

**Status:** ‚úÖ Ju≈º skonfigurowane!

W `manifest-06-plusworkflow-services.yaml` masz:
```yaml
sessionAffinity: ClientIP
```

To zapewnia, ≈ºe ten sam klient zawsze trafia do tej samej instancji PlusWorkflow, co jest wymagane dla cache.

**Sprawdzenie:**
```bash
kubectl get svc plusworkflow-lb -n plusworkflow -o yaml | grep sessionAffinity
```

---

## üöÄ Plan Wdro≈ºenia

### Krok 1: Backup Obecnych Danych
```bash
# Rƒôczny backup przed zmianami
kubectl apply -f manifest-09-postgres-backup-cronjob.yaml
kubectl create job --from=cronjob/postgres-backup postgres-backup-before-migration -n plusworkflow
```

### Krok 2: Instalacja Storage Class dla RWX
```bash
# Wybierz jednƒÖ opcjƒô:
# A) Longhorn (najprostsze)
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/deploy/longhorn.yaml

# B) NFS Provisioner (je≈õli masz NFS server)
helm install nfs-subdir-external-provisioner ...
```

### Krok 3: Zmiana PlusWorkflow na RWX
```bash
# Edytuj manifest-07-plusworkflow-statefulset.yaml
# Zmie≈Ñ storageClassName na: longhorn (lub nfs-client)

# Usu≈Ñ obecny StatefulSet (zachowaj PVC je≈õli chcesz zachowaƒá dane)
kubectl delete statefulset plusworkflow -n plusworkflow

# Zastosuj nowy manifest
kubectl apply -f manifest-07-plusworkflow-statefulset.yaml
```

### Krok 4: Replikacja PostgreSQL (opcjonalne)
```bash
# Opcja A: CloudNativePG (ZALECANE)
kubectl apply -f https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.22/releases/cnpg-1.22.0.yaml
# Nastƒôpnie utw√≥rz Cluster manifest (zobacz przyk≈Çad powy≈ºej)

# Opcja B: Prosty streaming replication
# 1. Utw√≥rz u≈ºytkownika replicator w PostgreSQL
# 2. Zastosuj manifest-08-postgres-replication-simple.yaml
```

---

## ‚ö†Ô∏è Uwagi i Ostrze≈ºenia

1. **ReadWriteMany wymaga odpowiedniego storage class** - `local-path` nie dzia≈Ça!
2. **Replikacja PostgreSQL** - CloudNativePG jest prostszy ni≈º Zalando Operator
3. **Backupy** - Regularnie sprawdzaj czy dzia≈ÇajƒÖ: `kubectl get cronjob -n plusworkflow`
4. **Sticky sessions** - Ju≈º skonfigurowane, nie wymaga zmian
5. **Testowanie** - Przetestuj failover i restore w ≈õrodowisku testowym przed produkcjƒÖ

---

## üìö Przydatne Linki

- **CloudNativePG:** https://cloudnative-pg.io/
- **Longhorn:** https://longhorn.io/
- **NFS Provisioner:** https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner
- **PostgreSQL Streaming Replication:** https://www.postgresql.org/docs/current/high-availability.html

